# Week 5 write-up

## Administrative matters
- Week 6 Quiz 1 postponed to Week 7 (10/30)
- Week 10 Midterm postponed to Week 11 (11/27) 

## Merge Sort and Asymptotic Analysis (åˆä½µæ’åºåŠæ¼¸é€²åˆ†æ)

### Merge
- **Merge** is the **core part** of Merge Sort
- Pseudo code:
<br><img src="Week 5\merge_pseudo.PNG" width="550px" />

> For the arrays in this book, the array index starts from 1

- Trace code (Demonstrate only for Array A4):
```
0   Merge(Array A4, 1, 1, 2)
1   n1 = 1 - 1 + 1 = 1
2   n2 = 2 - 1 = 1
3   L[1, 2]  R[1, 2]
4   i = 1
5   L[1] = A[1+1-1] = 5
6   j = 1
7   R[1] = A[1+1] = 2
8   L[2] = âˆ
9   R[2] = âˆ
10  i = 1
11  j = 1
12  k = 1
13  (L[1]=5) >= (R[1]=2), go to line 16
16  A[1] = R[1] = 2
17  j = 2, go to line 12
-----
12  k = 2
13  (L[1]=5) <= (R[2]=âˆ)
14  A[2] = L[1] = 5
15  i = 2, Array A4 has been sorted
```
- Array *A[5, 2, 4, 6, 1, 3, 2, 6]*
- *q* = (*p* + *r*) // 2
    - *q* = (1 + 8) // 2 = 4
- Pseudo code line 13 **must add the equal sign**, **otherwise** the algorithm will become **unstable**

### Recurrence
- Merge sort is a **recursive algorithm**
- Recurrence for merge sort:
<br><img src="Week 5\mergeSortRecurrence.PNG" width="550px" />

### Recursion Tree for Asymoptotic Analysis (éæ­¸æ¨¹ç”¨æ–¼æ¼¸è¿‘åˆ†æ)
<img src="Week 5\mergeSortRecursionTree.PNG" width="550px" />

- *Î¸(n lg n)* grows more slowly than *Î¸(n<sup>2</sup>)*
    - The larger the *n*, the greater the time complexity difference between the two algorithms
- Thus merge sort asymptotically beats insertion sort in the worst case (å› æ­¤ï¼Œåœ¨æœ€å£çš„æƒ…æ³ä¸‹ï¼ŒMerge sort æ™‚é–“è¤‡é›œåº¦æœƒæ¯” Insertion sort ä¾†å¾—å¥½)

### Tree height calculation
<img src="Week 5\lgn.PNG" width="550px" />

### Insertion Sort vs. Merge Sort
| | Insertion Sort | Merge Sort |
|-|---|---|
| Approach | Incremental (æ¼¸å¢å¼) | Divide-and-conquer (åˆ†æ²»æ³•) |
| Runtime Complexity | *O(n<sup>2</sup>*) | *Î¸(n lg n)* |
| Stable? | Yes | Yes |
| In-place? | Yes | No |
| **Note** | Can only be represented by *O* | Can use *O* or *Î¸* |
- **In-place**: Only a constant # of variables are stored outside the working array (in-place algorithm æ˜¯ä¸€ç¨®ä½¿ç”¨å°çš„ã€å›ºå®šæ•¸é‡çš„ç©ºé–“ä¾†è½‰æ›è³‡æ–™çš„ç®—æ³•ï¼Œç•¶ç®—æ³•åŸ·è¡Œæ™‚ï¼Œè¼¸å…¥çš„è³‡æ–™é€šå¸¸æœƒè¢«è¦è¼¸å‡ºçš„éƒ¨ä»½è¦†è“‹æ‰ã€‚ç°¡å–®ä¾†èªªå°±æ˜¯ä¸èƒ½é–‹æ–°çš„è¨˜æ†¶é«”ç©ºé–“å­˜æ”¾è³‡æ–™)
- **Stable**: Numbers with the same value in the output array are in the same order as the input array
    - Example:
        - Output: N T<sub>1</sub> U S T<sub>2</sub> is stable
        - Output: N T<sub>2</sub> U S T<sub>1</sub> is nonstable
    - The importance of stable varies by event

### Analyzing Divide-and-Conquer Algorithms
- Recurrence for a divide-and-conquer algorithms:
<br><img src="Week 5\Divide_and_conquer.PNG" width="550px" />

    - *a*: # of subproblems (åˆ‡æˆ # å€‹å­å•é¡Œ)
    - *n/b*: size of the subproblems (ä»”å•é¡Œçš„å¤§å°)
    - *D(n)*: time to divide the problem of size *n* into subproblems (å°‡å¤§å°ç‚º *n* çš„å•é¡Œåˆ†è§£ç‚ºå­å•é¡Œï¼Œæ‰€èŠ±çš„æ™‚é–“)
    - *C(n)*: time to combine the subproblem solutions to get the answer for the problem of size *n* (çµåˆ *n* å€‹å­å•é¡Œï¼Œæ‰€èŠ±çš„æ™‚é–“)
        - Note: *a* is not necessarily equal to *b*

- Merge sort:
    - *D(n)* = *Î¸(1)*: compute midpoint of array
    - *C(n)* = *Î¸(n)*: merging by scanning sorted subarrays **(Merge function)**

### Solving Recurrences
- Three general methods for solving recurrences
    - **Iteration**: Convert the recurrence into a summation by expanding some terms and then bound the summation (æš´åŠ›æ³•)
    - **Substitution**: Guess a solution and verify it by induction (æ•¸å­¸æ­¸ç´æ³•)
    - **Master Theorem**: If the recurrence has the form ***T(n) = aT(n/b) + f(n)***, then most likely there is a formula that can be applied (ä¸»å®šç†) **(â­Compulsory exam)**
- Two simplifications that won't affect asymptotic analysis (ä¸æœƒå½±éŸ¿æ™‚é–“è¤‡é›œåº¦çš„ç°¡åŒ–)
    - Ignore floors and ceilings
    - Assume base cases are constant, i.e., *T(n) = Î¸(1)* for small *n*

### Solving Recurrences: Iteration
- Example: *T(n) = 4T(n/2) + n*
<br><img src="Week 5\iteration.PNG" width="550px" />

- Sometimes when master theorem cannot be solved, you can try to use iteration method

### Iteration by Using Recursion Trees
- **Root: computation *(D(n) + C(n))* at top level of recursion**
- Node at level *i*: Subproblem at level *i* in the recursion
- Height of tree: (#levels - 1) in the recursion
- *T(n)* = sum of all nodes in the tree, *T(1) = 1*
- *T(n) = 4T(n/2) + n = n + 2n + 4n + â€¦ + 2<sup>lgn</sup>n = Î¸(n<sup>2</sup>)*
<br><img src="Week 5\iterationTree.PNG" width="550px" />

### Solving Recurrences: Master Theorem **(â­Compulsory exam)**
- Let **a â‰¥ 1** and **b > 1** be constants, *f(n)* be an asymptotically positive function, and *T(n)* be defined on nonnegative integers as ***T(n) = aT(n/b) + f(n)***
- Then, *T(n)* can be bounded asymptotically as follows:
    - *T(n) = Î¸(n<sup>log<sub>b</sub>a</sup>)* if *f(n) = O(n<sup>log<sub>b</sub>a-âˆˆ</sup>)* for some constant âˆˆ > 0
    - *T(n) = Î¸(n<sup>log<sub>b</sub>a</sup> lg n)* if *f(n) = Î¸(n<sup>log<sub>b</sub>a</sup>)*
    - *T(n) = Î¸(f(n))* **if *f(n) = Î©(n <sup>log<sub>b</sub>a+âˆˆ</sup>)*** for some constant *âˆˆ* > 0 and ***af(n/b) â‰¦ cf(n)* for some constant *c* < 1** and all sufficiently large *n*
- **Intuition (ç›´è¦ºã€ç›´è§€): compare *f(n)* with *Î¸(n<sup>log<sub>b</sub>a</sup>)***
    - Case 1: *f(n)* is polynomially (å¤šé …å¼) smaller than *Î¸(n<sup>log<sub>b</sub>a</sup>)* 
        - **(*f(n) <  Î¸(n<sup>log<sub>b</sub>a</sup>)*)**
    - Case 2: *f(n)* is asymptotically (æ¼¸é€²åœ°) equal to *Î¸(n<sup>log<sub>b</sub>a</sup>)* 
        - **(*f(n) =  Î¸(n<sup>log<sub>b</sub>a</sup>)*)**
    - Case 3: *f(n)* is polynomially larger than *Î¸(n<sup>log<sub>b</sub>a</sup>)* 
        - **(*f(n) >  Î¸(n<sup>log<sub>b</sub>a</sup>)*)**
### Examples of Master Theorem **(â­Compulsory exam)**
- Eg. 1: *T(n) = 4T(n/2) + n*
    - Compare *f(n) = n* with *Î¸(n<sup>log<sub>b</sub>a</sup>) = n<sup>2</sup>*: *f(n) = n = O(n<sup>2-âˆˆ</sup>)*
    - Case 1 applies: *T(n) = Î¸(n<sup>log<sub>b</sub>a</sup>) = Î¸(n<sup>2</sup>)*
- Eg. 2: *T(n) = 2T(n/2) + n*
    - Compare *f(n) = n* with *n<sup>log<sub>b</sub>a</sup> = n*: *f(n) = n = Î¸(n)*
    - Case 2 applies: *T(n) = Î¸(n<sup>log<sub>b</sub>a</sup> lg n) = Î¸(n lg n)*
- Eg. 3: *T(n) = 3T(n/4) + n lg n*
    - Compare *f(n) = n lg n* with *n<sup>log<sub>b</sub>a</sup> = n<sup>0.79</sup>*: *f(n) = n lg n = Î©(n<sup>0.79+âˆˆ</sup>)*
    - Case 3 could apply: Need to **check for "regularity" condition** that *af(n/b) â‰¦ cf(n)*
        - Find c < 1 subject to (æ»¿è¶³) *af(n/b) â‰¦ cf(n)* for large enough *n*
        - *3(n/4) lg (n/4) â‰¦ cn lg n* which is true for *c = 3/4*
    - Case 3 applies: *T(n) = Î¸(f(n)) = Î¸(n lg n)*

### Examples of Master Theorem (cont'd)
- *T(n) = 2<sup>n</sup>T(n/2) + n<sup>n</sup>*
    - Master theorem can't be applied, **because *a* is not constant**
- *T(n) = 0.5T(n/2) + 1/n*
    - Master theorem can't be applied, **because *a* < 1**
- *T(n) = 64T(n/8) - n<sup>2</sup>log n*
    - Master theorem can't be applied, **because *f(n)* is not positive**
- *T(n) = 4T(n/2) + n<sup>2</sup>lg n*
    - Master theorem can't be applied, **because**
        - *n<sup>log<sub>b</sub>a</sup> = n<sup>2</sup>*, *f(n) = n<sup>2</sup> lg n*, case 3 could apply
        - **Regularity check**: *4(n/2)<sup>2</sup> lg(n/2) â‰¦ cn<sup>2</sup> lg n*, arrange the formula â†’ *n<sup>2</sup> lg(n/2) â‰¦ cn<sup>2</sup> lg n*, **because c = 1, so master theorem can't be applied**

## Unit 2: Sorting and Order Statistics
- Course contents:
    - Heapsort (å †ç©æ’åºæ³•)
    - Quicksort (å¿«é€Ÿæ’åºæ³•)
    - Sorting in linear time
    - Order statistics (Won't teach this semester, because the time is not enough)

## Heapsort (å †ç©æ’åºæ³•)
### Tree Height and Depth
- **Height** of a node *u*: Length of the longest path from *u* to a leaf **(The number in the circle)**
- **Depth** of a node *u*: Length of the path from the root to *u*
- **Height of a tree**: Maximum depth of its nodes
- A **level** is the set of all nodes at the same depth **(level = depth + 1)**
<br><img src="Week 5\TreeDefine.PNG" width="550px" />

### Binary Trees
- A **full** binary tree: every node has either **0 or 2 children**
- A **complete** binary tree: the lowest *d*-1 levels of a binary tree of height *d* are filled and level *d* is partially filled from left to right (ç°¡å–®ä¾†èªªï¼Œå¦‚æœä¸€æ£µæ¨¹çš„ node æŒ‰ç…§ Full Binary Tree çš„æ¬¡åºæ’åˆ— (ç”±ä¸Šè‡³ä¸‹ï¼Œç”±å·¦è‡³å³)ï¼Œå‰‡ç¨±æ­¤æ¨¹ç‚º Complete Binary Tree)
- A **perfect** binary tree: all *d* levels of a heught-*d* binary tree are filled.
<br><img src="Week 5\three_trees.PNG" width="550px" />

### Binary Heap
- Binary heap data structure: represented by an array *A*
    - Complete binary tree
    - **Max-Heap property**: A node's key **â‰§** its children's keys
    <br><img src="Week 5\max_heap.PNG" width="550px" />
    - **Min-Heap property**: A node's key **â‰¦** its children's keys
- Implementation
    - Root: *A[1]*
    - For *A[i]*, LEFT child is *A[2i]*, RIGHT child is *A[2i+1]*, and PARENT is *A[âŒŠğ‘–/2âŒ‹]*
    <br><img src="Week 5\max_heap.PNG" width="550px" />
        - If you want to use C language code to implementation:
            - Root: *A[0]*
            - LEFT child is *A[2i+1]*
            - RIGHT child is *A[2i+2]*
    - A.heap-size (# of elements in the heap stored within *A*) â‰¦ *A*.length (# of elements in *A*)